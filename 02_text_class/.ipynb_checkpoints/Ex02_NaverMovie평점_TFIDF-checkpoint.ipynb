{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 자료 출처 ] 네이버 평점 자료\n",
    "\n",
    "https://github.com/e9t/nsmc\n",
    "\n",
    "\n",
    "[ 예제 출처 ] 도서 - 파이썬 머신러닝 완벽 가이드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이타 확인\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('nsmc-master/ratings_train.txt', sep='\\t')\n",
    "train_df.head(5)\n",
    "\n",
    "# label 값이 1이면 긍정, 0이면 부정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    75173\n",
       "1    74827\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0과 1 (부정과 긍정)의 비율이 균등한 분포임을 확인\n",
    "train_df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    5\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null 값 확인\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'document' 컬럼에 Null 값이 있는데 이는 공백으로 변환\n",
    "train_df = train_df.fillna(' ')\n",
    "\n",
    "# 숫자를 공백으로 변경\n",
    "import re\n",
    "train_df['document'] = train_df['document'].apply(lambda x: re.sub(r\"\\d+\",' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이타로 위와 동일한 작업\n",
    "test_df = pd.read_csv('nsmc-master/ratings_test.txt', sep='\\t')\n",
    "test_df = test_df.fillna(' ')\n",
    "test_df['document'] = test_df['document'].apply(lambda x: re.sub(r\"\\d+\",' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\java\\anaconda3\\lib\\site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\java\\anaconda3\\lib\\site-packages (from konlpy) (4.6.1)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\java\\anaconda3\\lib\\site-packages (from konlpy) (1.2.0)\n",
      "Collecting tweepy>=3.7.0\n",
      "  Downloading tweepy-3.10.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting beautifulsoup4==4.6.0\n",
      "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\java\\anaconda3\\lib\\site-packages (from konlpy) (1.19.2)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\java\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\java\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (2.24.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\java\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\java\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\java\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\java\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\java\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy, beautifulsoup4, konlpy\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.9.3\n",
      "    Uninstalling beautifulsoup4-4.9.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.9.3\n",
      "Successfully installed beautifulsoup4-4.6.0 konlpy-0.5.2 oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.10.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\simmi\\anaconda3\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\simmi\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\simmi\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\simmi\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\simmi\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.5.2)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1321 sha256=672854ac9a7da500f7e251239051dba9953b952c0bdc0de13f70ebec9ffca8f1\n",
      "  Stored in directory: c:\\users\\simmi\\appdata\\local\\pip\\cache\\wheels\\22\\0b\\40\\fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석\n",
    "# Twitter 객체의 morphs() 는 입력 인자로 들어온 문장을 형태소 단어 형태로 토큰화해 리스트 객체로 반환\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 인자로 들어온 텍스트를 형태소 단어로 토큰화하여 리스트 형태로 변환\n",
    "def tw_tokenizer(text):\n",
    "    tokens_ko = twitter.morphs(text)\n",
    "    return tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 분석\n",
    "\n",
    "- 텍스트 분석은  비정형 데이타인 텍스트를 벡터값을 가지는 피처로 변환하는 것이 중요하다\n",
    "- 숫자형의 피처 기반 데이타만 머신러닝 알고리즘이 처리하기 때문에 텍스트를 의미 있는 피처 형태로 추출해야 한다\n",
    "\n",
    "\n",
    "\n",
    "### [ TF-IDF 참고 ](https://soojle.gitbook.io/project/undefined-2/nlp/tf-idf-term-frequency-inverse-document-frequency)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 사이킷런에서 TF-IDF 벡터화 - TfidfVectorizer 클래스 이용\n",
    "\n",
    "\n",
    "+ max_df : 전체 문서에 걸쳐서 너무 높은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터\n",
    "         너무 높은 빈도수를 가지는 단어는 문법적인 특성으로 반복되는 단어일 가능성이 높기에 이를 제거\n",
    "         max_df=100 같이 정수값이면 전체 문서에서 100개 이하로 나타나는 단어만 피처로 추출한다\n",
    "         max_df=0.9 같이 실수값이면 전체 문서에 걸쳐 빈도수가 0~95%까지의 단어만 피처로 추출하고 나머지 상위 5%는 추출하지 않는다\n",
    "         \n",
    "+ min_df : 전체 문서에 걸쳐서 너무 낮은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터\n",
    "         너무 낮은 빈도수의 단어는 중요하지 않을 단어일 가능성이 높기에 이를 제거\n",
    "         mix_df=10 같이 정수값이면 전체 문서에서 10번 이하로 나타나는 단어로 피처로 추출하지 않는다\n",
    "         mix_df-0.02 같이 실수값이면 전체 문서에 하위 2% 이하의 빈도수를 가지는 단어는 추출하지 않는다\n",
    "\n",
    "+ max_features : 추출하는 피처의 개수를 제한\n",
    "         max_features=2000 이라면 가장 높은 빈도수를 가지는 단어 순으로 정렬해 2000개 까지만 추출\n",
    "         \n",
    "+ stop_words : 'english'로 지정하면 영어의 스톱워드로 지정된 단어는 추출에서 제외\n",
    "         stop_words 는 분석에 큰 의미가 없는 단어를 지칭\n",
    "\n",
    "+ n_gram_range : 예를 들어 (1,2)로 지정하면 토큰화된 단어를 1개(minimun) 그리고 순서대로 2개씩(maximun) 묶어서 피처로 추출한다\n",
    "\n",
    "\n",
    "+ tokenizer : 토큰화를 별도로 커스텀 함수 이용시 지정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### * 텍스트 데이타의 피처 벡터화 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# 시간소요 ( 노트북 16G : 30 분이상)\n",
    "# TF-IDF 방식으로 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
    "# 상위 90%로 제한\n",
    "tfidf_vect.fit(train_df['document'])\n",
    "tfidf_matrix_train = tfidf_vect.transform(train_df['document'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 학습 데이타에 적용한 TfidfVectorizer를 테스트 데이터에도 사용 ( 시간소요 : 10분)\n",
    "tfidf_matrix_test = tfidf_vect.transform(test_df['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 회귀를 이용해 감성 분석 분류 ( 학습/예측/평가수행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lg_clf = LogisticRegression(random_state=0)\n",
    "\n",
    "# -------------- 모델에 학습\n",
    "lg_clf.fit(tfidf_matrix_train, train_df['label'])\n",
    "\n",
    "# -------------- 테스트를 예측\n",
    "preds = lg_clf.predict(tfidf_matrix_test)\n",
    "\n",
    "# --------------- 정확도\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('TF-IDF LogisticRegression의 예측 정확도 : ', accuracy_score(test_df['label'], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV를 이용하여 로지스틱 회귀의 하이퍼 파라미터 최적화를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   54.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3.5} 0.8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simmi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 파라미터 C 최적화를 위해 GridSearchCV를 이용\n",
    "params = {'C':[1, 3.5, 4.5, 5.5, 10]}\n",
    "grid_cv = GridSearchCV(lg_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv.fit(tfidf_matrix_train, train_df['label'])\n",
    "print(grid_cv.best_params_, round(grid_cv.best_score_,4))\n",
    "\n",
    "# [결과] {'C': 3.5} 0.8592\n",
    "# C가 3.5일 때 0.8592 정확도임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 정확도:  0.86184\n"
     ]
    }
   ],
   "source": [
    "# classifier는 GridSearchCV에서 최적 파라미터로 학습된 classifier를 그대로 이용\n",
    "best_estimator = grid_cv.best_estimator_\n",
    "preds = best_estimator.predict(tfidf_matrix_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('LogisticRegression 정확도: ', accuracy_score(test_df['label'], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 댓글로 부정인지 긍정인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(test_df['document']) -> Series\n",
    "comment = pd.Series(['안보면 후회'])\n",
    "tfidf_matrix_test = tfidf_vect.transform(comment)\n",
    "preds = best_estimator.predict(tfidf_matrix_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comment = pd.Series(['감독 꼬라지하고는'])\n",
    "tfidf_matrix_test = tfidf_vect.transform(comment)\n",
    "preds = best_estimator.predict(tfidf_matrix_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = pd.Series(['이 훌륭한 배우들을 데려다가'])\n",
    "tfidf_matrix_test = tfidf_vect.transform(comment)\n",
    "preds = best_estimator.predict(tfidf_matrix_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = pd.Series(['이 훌륭한 배우들을 데려다가 이렇게 만들었어'])\n",
    "tfidf_matrix_test = tfidf_vect.transform(comment)\n",
    "preds = best_estimator.predict(tfidf_matrix_test)\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
